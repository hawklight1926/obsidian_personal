---
title: "Fast LLM Post-training via Decoupled and Best-of-N Speculation"
url: "https://arxiv.org/abs/2511.16193"
date: "2025-11-24"
updated: ""
category: "AI"
tags: []
authors: "Rongxin Cheng, Kai Zhou, Xingda Wei, Siyuan Liu, Mingcong Han, Mingjing Ai, Yeju Zhou, Baoquan Zhong, Wencong Xiao, Rong Chen, Haibo Chen"
image: ""
memo: ""
read: false
ignored: false
pinned: false
---

## 要約
この論文は、大規模言語モデル（LLM）の学習後プロセスを高速化する新しい手法を提案しています。具体的には、「Decoupled（分離型）」と「Best-of-N Speculation（N個の中から最良のものを推測する）」というアプローチを採用することで、LLMの効率的なポストトレーニングを実現します。

本研究は、コンピューターサイエンスの分散、並列、クラスターコンピューティングの分野に属し、LLM運用のパフォーマンス向上を目指しています。
