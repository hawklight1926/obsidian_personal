---
title: "MiniLLM: Knowledge Distillation of Large Language Models"
url: "https://arxiv.org/abs/2306.08543"
date: "2025-11-24"
updated: ""
category: "AI"
tags: []
authors: "Yuxian Gu, Li Dong, Furu Wei, Minlie Huang"
image: ""
memo: ""
read: false
ignored: false
pinned: false
---

## 要約
本論文は、大規模言語モデル（LLM）の知識をより小型のモデルへ効率的に転移させる「知識蒸留」に関する研究です。
具体的には「MiniLLM」という新しいフレームワークを提案しており、教師となるLLMの知識を生徒モデルへ効果的に蒸留します。
この手法により、LLMの推論性能を維持しつつ、モデルサイズと計算コストを大幅に削減し、より効率的な運用を可能にすると期待されます。
