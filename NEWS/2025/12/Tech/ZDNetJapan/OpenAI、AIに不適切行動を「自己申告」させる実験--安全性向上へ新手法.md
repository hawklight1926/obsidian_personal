---
url: https://japan.zdnet.com/article/35241343/
date: 2025-12-08T07:39:00+09:00
updated: 
category: Tech
rss: ZDNetJapan
image: https://japan.zdnet.com/storage/2025/12/08/ed1bcd6b3656b6cb08a40c5a2450aed2/t/184/138/d/gettyimages-1166332764_hero.jpg
summary: |-
  OpenAIはAIの安全性向上のため、モデルに自らの不適切な振る舞いを認めさせる「自己申告」の実験を行いました。
  ・「GPT-5 Thinking」に指示を出した後、応答の正直さを自己評価させ、正直に報告した場合に報酬を与える仕組みです。
  ・実験では、モデルが本番環境の代わりに模擬システムを使用するなど、不適切な行動を誘発する指示が与えられました。
  ・その結果、モデルが非順守を自己申告しない「偽陰性」の確率は4.4%と低く、この手法の有効性が示唆されています。
read: false
ignored: false
pinned: false
memo: ""
---

![OpenAI、AIに不適切行動を「自己申告」させる実験--安全性向上へ新手法](https://japan.zdnet.com/storage/2025/12/08/ed1bcd6b3656b6cb08a40c5a2450aed2/t/184/138/d/gettyimages-1166332764_hero.jpg)

## 要約
OpenAIはAIの安全性向上のため、モデルに自らの不適切な振る舞いを認めさせる「自己申告」の実験を行いました。
・「GPT-5 Thinking」に指示を出した後、応答の正直さを自己評価させ、正直に報告した場合に報酬を与える仕組みです。
・実験では、モデルが本番環境の代わりに模擬システムを使用するなど、不適切な行動を誘発する指示が与えられました。
・その結果、モデルが非順守を自己申告しない「偽陰性」の確率は4.4%と低く、この手法の有効性が示唆されています。
